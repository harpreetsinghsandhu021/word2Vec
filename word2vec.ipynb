{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fd06ae-9902-4561-a639-7c54e1a3d5ed",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5146b-64b1-4517-8112-d3e79a50faad",
   "metadata": {},
   "source": [
    "word2vec is a singular algorithmn, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n",
    "\n",
    "This most popular two methods for learning representations of words are: \n",
    "\n",
    "- **Continuous bag of words model**: It predicts the middle word based on surrounding context words. The context consists of a few words before and after the middle word. This architecture is called a bag-of-words model as the order of words in the context is not important. \n",
    "- **Continuous skip-gram model**: It predicts words within a certain range before and after the current word in the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db60bc-88f0-4b42-9f1e-61db8a0a4182",
   "metadata": {},
   "source": [
    "## Skip-gram and negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29300516-db54-4360-a6f5-1e42334fd693",
   "metadata": {},
   "source": [
    "Skip-gram is a technique used in natural language processing to learn word representations (word embeddings). Imagine you're trying to understand words by looking at their neighbors.\n",
    "Here's how Skip-gram works:\n",
    "\n",
    "- Take a word in a sentence\n",
    "- Try to predict the words that are likely to appear around it\n",
    "- Example: In the sentence \"The cat sits on the mat\"\n",
    "  - If we're looking at the word \"cat\", the model tries to predict nearby words like \"the\", \"sits\"\n",
    "\n",
    "\n",
    "\n",
    "Negative sampling is a clever trick to make this learning process more efficient:\n",
    "\n",
    "- Instead of looking at every single word in the vocabulary (which would be super slow)\n",
    "- The model randomly selects a few \"negative\" words that are unlikely to appear near the target word\n",
    "- This helps the model learn to distinguish between words that are likely and unlikely to be context words\n",
    "\n",
    "Think of it like a game:\n",
    "\n",
    "- Positive example: \"cat\" is near \"sits\" ✓\n",
    "- Negative examples: \"cat\" is probably NOT near \"computer\" or \"rocket\" ✗\n",
    "\n",
    "The magic happens when the model learns to:\n",
    "\n",
    "- Recognize which words are likely to be together\n",
    "- Create vector representations that capture word meanings\n",
    "- Do this efficiently by only checking a few random words instead of all possible words\n",
    "\n",
    "Essentially, Skip-gram with negative sampling is a smart way to teach computers to understand word relationships by looking at how words typically appear together in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "501b3141-7e33-405c-97b2-440788461d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cf33e-2af6-4643-9d21-a0b48af03c45",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ce50da-313a-4862-85c1-22db208695f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import re\n",
    "import string \n",
    "import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377e7d82-a367-4714-8815-c466951784f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcd0baf-5ee0-45a1-a750-93e4c1dfc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c55ea-cfd4-416c-8d38-0c7749634ebd",
   "metadata": {},
   "source": [
    "### Vectorize an example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d39f0e-9bbe-44b5-aefc-ad9612a904ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The wide road shimmered in the hot sun'\n",
    "tokens = list(sentence.lower().split())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47160f2-8d1e-45bc-a590-a588d154140e",
   "metadata": {},
   "source": [
    "Create a vocabulary to save mappings from tokens to integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a845061-ebbf-469e-9fae-58fa573f2d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 'the': 1,\n",
       " 'wide': 2,\n",
       " 'road': 3,\n",
       " 'shimmered': 4,\n",
       " 'in': 5,\n",
       " 'hot': 6,\n",
       " 'sun': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "index = 1 \n",
    "vocab['<pad>'] = 0 # add a padding token\n",
    "\n",
    "for token in tokens: \n",
    "    if token not in vocab: \n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc19364-46f0-4cd6-bec6-30454b348742",
   "metadata": {},
   "source": [
    "Create an inverse vocabulary to save mappings from integer indices to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eff8269-a246-4098-afc1-f2677c2d4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'the',\n",
       " 2: 'wide',\n",
       " 3: 'road',\n",
       " 4: 'shimmered',\n",
       " 5: 'in',\n",
       " 6: 'hot',\n",
       " 7: 'sun'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = {}\n",
    "for token, index in vocab.items():\n",
    "    inverse_vocab[index] = token\n",
    "inverse_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27aeda2-f970-445a-ad93-80be561015cc",
   "metadata": {},
   "source": [
    "### Vectorize the Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfb7be5-0b38-4e6e-bf05-c46098fdb4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 6, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sequence = []\n",
    "\n",
    "for word in tokens: \n",
    "    example_sequence.append(vocab[word])\n",
    "example_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a039e5-3cc7-43c3-9cc6-4de2037f69f7",
   "metadata": {},
   "source": [
    "### Generate skip-grams from one sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9c12e-780b-4f00-ba64-f8b9264af4ed",
   "metadata": {},
   "source": [
    "The tf.keras.preprocessing.sequence module provides useful functions that simplify data preparation for word2vec. You can use the tf.keras.preprocessing.sequence.skipgrams to generate skip-gram pairs from the example_sequence with a given window_size from tokens in the range [0, vocab_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738957d0-d857-4d67-9d91-36c24a998d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    example_sequence, \n",
    "    vocabulary_size=vocab_size, \n",
    "    window_size=window_size, \n",
    "    negative_samples=0\n",
    ")\n",
    "\n",
    "len(positive_skip_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbbb21-03cb-496e-a824-84e223edc7b1",
   "metadata": {},
   "source": [
    "print a few positive skip grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e252ca-0934-4e4a-854c-2f698472c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1): (sun, the)\n",
      "(5, 4): (in, shimmered)\n",
      "(2, 4): (wide, shimmered)\n",
      "(6, 1): (hot, the)\n",
      "(5, 1): (in, the)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]: \n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564be55-91d6-4e1d-8125-131c0fc34625",
   "metadata": {},
   "source": [
    "### Negative sampling for one skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614140e5-ce2d-49cd-a709-a826a97d7a04",
   "metadata": {},
   "source": [
    "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would server as negative samples for training, you need to sample random words from the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e958aa69-c7c6-4bfa-bec9-973fce87ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
      "['wide', 'the', 'shimmered', 'road']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 07:57:25.522669: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-08 07:57:25.522740: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-12-08 07:57:25.522757: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-12-08 07:57:25.523048: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-08 07:57:25.523089: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "num_ns = 4 # the number of negative samples per positive context\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype='int64'),(1, 1))\n",
    "\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class, # class that should be sampled as positive\n",
    "    num_true=1, # each positive skip gram has 1 positive context class \n",
    "    num_sampled=num_ns, # number of negative context words to sample\n",
    "    unique=True, # all the negative samples should be unique \n",
    "    range_max=vocab_size, # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED, \n",
    "    name='negative_sampling'\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bd7f8-06b5-4417-92c4-fcb665ddbd41",
   "metadata": {},
   "source": [
    "### Constructing One Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea11d17-6bb3-49d0-b652-e80177d72e5f",
   "metadata": {},
   "source": [
    "For a given positive (target_word, context_word) skip-gram, you now also have num_ns negative sampled context words that do not appear in the window size neighborhood of target_word. Batch the 1 positive context_word and num_ns negative context words into one tensor. This produces a set of positive skip-grams (labeled as 1) and negative samples (labeled as 0) for each target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc687cc-6551-4e7b-9e91-afbe115ad3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezed_context_class = tf.squeeze(context_class, 1) # Reduce a dimension so you can use concatenation in the next step\n",
    "squeezed_context_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99fea88-ca3c-467b-8027-14c46a51bc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 2, 1, 4, 3])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0) # concatenate a positive context word with negative sampled words\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f7a4a4-816d-4baa-ba77-590fd4946000",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.constant([1] + [0]*num_ns, dtype='int64') # label the first context word as `1` (positive) followed by `num_ns` 0`s negative\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f545a4-7238-49c6-bb7f-acc360e87b16",
   "metadata": {},
   "source": [
    "Check out the context and the corresponding labels for the target word from the skip-gram example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90787d73-8d03-4545-9b93-e2ccd947ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 7\n",
      "target_word     : sun\n",
      "context_indices : [1 2 1 4 3]\n",
      "context_words   : ['the', 'wide', 'the', 'shimmered', 'road']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0573b50-9d6c-4dac-b86d-c310a5969eb5",
   "metadata": {},
   "source": [
    "A tuple of (target, context, label) tensors constitutes one training example for training your skip-gram negative sampling word2vec model. Notice that the target is of shape (1,) while the context and label are of shape (1+num_ns,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c00a114-cfb8-4ddf-a3c8-d1692c83e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : 7\n",
      "context : tf.Tensor([1 2 1 4 3], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c08722-3b48-49c9-9326-25ca231d02fa",
   "metadata": {},
   "source": [
    "### Compile all steps into one function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29cad736-abc1-43d1-a918-4745264cb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35ca2445-58b1-4b11-bab2-42063c68f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n",
       "       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67643507-d93d-441e-a05e-fdcde0a749b7",
   "metadata": {},
   "source": [
    "### Generate Training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08a14c60-6f10-4c42-a168-f7aa026f921e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function generates training data for the skip-gram model with negative sampling\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed): \n",
    "    \"\"\" \n",
    "    Parameters: \n",
    "    sequences: Input sequences (sentences encoded as integers)\n",
    "    window_size: size of the context window around each target word\n",
    "    num_ns: number of negative samples to generate\n",
    "    vocab_size: total number of unique words in the vocabulary\n",
    "    seed: random seed for reproducibility\n",
    "    \"\"\"\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Creates a sampling table to help with sub-sampling frequent words\n",
    "    # this reduces the probability of sampling very common words too often\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # tqdm provides a progress bar to track progress\n",
    "    for sequence in tqdm.tqdm(sequences): \n",
    "        # Generates positive skip-gram pairs for the current sequence\n",
    "        # skipgrams() creates context word pairs within the specified window size\n",
    "        # negative_samples=0 means only positive pairs are generated at this stage\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence, \n",
    "            vocabulary_size=vocab_size, \n",
    "            sampling_table=sampling_table, \n",
    "            window_size=window_size, \n",
    "            negative_samples=0\n",
    "        )\n",
    "\n",
    "        # Iterates through each positive skip-gram pair\n",
    "        for target_word, context_word in positive_skip_grams: \n",
    "            # Creates a tensor for the context word to prepare for negative sampling\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
    "\n",
    "            # Generates negative samples using log-uniform sampling\n",
    "            # Ensures sampled words are unique and within vocabulary range\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1, \n",
    "                num_sampled=num_ns, \n",
    "                unique=True, \n",
    "                range_max=vocab_size, \n",
    "                seed=seed, \n",
    "                name='negative_sampling'\n",
    "            )\n",
    "\n",
    "            # Combines the positive context word with negative samples \n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "\n",
    "            # Creates corresponding labels (1 for positive, 0 for negative samples)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ab27d-673d-4fcf-a5c4-1cb9374698c1",
   "metadata": {},
   "source": [
    "### Prepare training data for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52206b15-1fb6-4da6-81f8-82191683df02",
   "metadata": {},
   "source": [
    "#### Download text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2ff022d-2328-4c62-a132-32a5aa1f9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb4f2c24-d395-4d5c-a902-be81c9701e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7f979-60f4-4e49-8990-46afb450087a",
   "metadata": {},
   "source": [
    "Use the non empty lines to construct a tf.data.TextLineDataset object for the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "897a61f3-539d-450b-b4dd-f6145f3cfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ff051-50e9-49a1-b8d0-32c7bba16270",
   "metadata": {},
   "source": [
    "### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d40e4f9-db5f-476c-9b22-7eb525de4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5448df2b-f31b-4af3-856f-da09cff12464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ba674-a269-45d8-b18d-0c2a2bfc0ff0",
   "metadata": {},
   "source": [
    "Call TextVectorization.adapt on the text dataset to create vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb6f7745-126e-4f65-b62b-1bfb87b6652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 09:29:22.173756: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754408e-e6c2-46d5-a380-bc1adcc6da74",
   "metadata": {},
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with TextVectorization.get_vocabulary. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a59fff25-b087-4926-9b90-2f0dd37e1740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea2e66-b6ff-47b7-a45c-ceb6b83be347",
   "metadata": {},
   "source": [
    "The vectorize_layer can now be used to generate vectors for each element in the text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b2581cc-483f-44df-88ff-ea2be3e76db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be693c7-cf4d-45f6-b83c-94d637a02f5d",
   "metadata": {},
   "source": [
    "### Obtain Sequences from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0cbf41e-25cd-4417-8cf5-1af23f25109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 10:05:28.378658: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ede25af6-7993-47fc-9d10-35ba0f671a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[   7   41   34 1286  344    4  200   64    4 3690] => ['you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[1286 1286    0    0    0    0    0    0    0    0] => ['resolved', 'resolved', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[  89    7   93 1187  225   12 2442  592    4    2] => ['first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:10]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472cb47-30ff-4901-81c1-f109694320d0",
   "metadata": {},
   "source": [
    "### Generate training examples from sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1ca73-cbd6-42be-b5a0-ac9a2c562914",
   "metadata": {},
   "source": [
    "sequences is now a list of int encoded sentences. Just call the generate_training_data function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a158dfc-6ab9-4288-932f-e12a7d039aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████| 32777/32777 [01:02<00:00, 523.12it/s]"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences, \n",
    "    window_size=2, \n",
    "    num_ns=4, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4407666-b00b-457e-bdda-69dd65f1bc40",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb8c2fad-c368-42ca-bf89-d662d84a064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd681f85-68b0-4dcf-8a08-dfc5caf7f37e",
   "metadata": {},
   "source": [
    "Apply Dataset.cache and Dataset.prefetch to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a3499fc-576f-47c2-b0a1-30590fd9a769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44036ca-9171-4124-88e4-8f9e7a438db7",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a9901-7f63-48c5-9534-6668c0149784",
   "metadata": {},
   "source": [
    "The word2vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0713a7-b4b0-4d73-a785-a4c32a466a30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Subclassed word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a3e067d-fd7f-4bf9-a261-5cd317ed9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom keras model for the word2vec algorithm\n",
    "# Inherits from `tf.keras.model` to create a neural network model \n",
    "class Word2Vec(tf.keras.Model): \n",
    "    def __init__(self, vocab_size, embedding_dim): \n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        vocab_size: Total number of unique words in the vocabulary\n",
    "        embedding_dim: Dimension of the word embedding vector\n",
    "        \"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # Create two embedding layers \n",
    "        self.target_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name='w2v_embedding') #  Represents the input (target) words\n",
    "        self.context_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # Represents the context words\n",
    "\n",
    "    def call(self, pair): # This is a forward pass method that defines how input is processed\n",
    "        target, context = pair \n",
    "        if len(target.shape) == 2: \n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # Converts integer-encoded words to their embedding representations\n",
    "        word_emb = self.target_embedding(target) \n",
    "        context_emb = self.context_embedding(context)  # IMPORTANT: Changed from target_embedding to context_embedding\n",
    "        \n",
    "        # Uses Einstein summation (tf.einsum) to compute dot products\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb) \n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d65ab-3f64-4ed4-9e22-dd190a225785",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Define loss function and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fcc67f6-f5b6-40d3-bd43-3e0f3f27cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dde41ba2-b821-41a2-9e38-e530550bf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb201f47-374b-4732-8685-175292b453dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f38db3f-ac48-4261-a4df-264719eed8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:23:12.289828: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.2183 - loss: 1.6089\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5993 - loss: 1.5900\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6112 - loss: 1.5343\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5601 - loss: 1.4458\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5757 - loss: 1.3497\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6094 - loss: 1.2539\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6461 - loss: 1.1640\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6805 - loss: 1.0805\n",
      "Epoch 9/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7116 - loss: 1.0032\n",
      "Epoch 10/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7398 - loss: 0.9317\n",
      "Epoch 11/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7654 - loss: 0.8655\n",
      "Epoch 12/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7874 - loss: 0.8044\n",
      "Epoch 13/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8071 - loss: 0.7482\n",
      "Epoch 14/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8233 - loss: 0.6967\n",
      "Epoch 15/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8380 - loss: 0.6494\n",
      "Epoch 16/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8515 - loss: 0.6063\n",
      "Epoch 17/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8633 - loss: 0.5669\n",
      "Epoch 18/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8745 - loss: 0.5310\n",
      "Epoch 19/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8847 - loss: 0.4982\n",
      "Epoch 20/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8932 - loss: 0.4684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x34713e1c0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d667f633-89ce-4a99-9102-e4af88445ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-69a734aa387b0f24\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-69a734aa387b0f24\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d980d-ca58-40ee-884d-007c9e64471d",
   "metadata": {},
   "source": [
    "### Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a3648-fd65-4ada-9e07-34875c17c83b",
   "metadata": {},
   "source": [
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cbb7b-eeb2-415d-9bee-1bd0eea81518",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5fdb2-6a89-4912-85f3-dfd358e286ff",
   "metadata": {},
   "source": [
    "Create and save the vectors and metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e14ec91c-de2d-4e18-a9e0-1b3fee5fd94f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip 0, it's padding.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m vec \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m[index]\n\u001b[1;32m      8\u001b[0m out_v\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m out_m\u001b[38;5;241m.\u001b[39mwrite(word \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word-embeddings",
   "language": "python",
   "name": "word-embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
